import streamlit as st
import pandas as pd
import plotly.express as px
import os
from PIL import Image as PILImage
import base64
import io
from io import BytesIO
import tempfile
from translate import Translator
from pandasai import SmartDataframe
from pandasai.responses.streamlit_response import StreamlitResponse
from pandasai.responses.response_parser import ResponseParser

from pandasai.engine import set_pd_engine
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
from openai import OpenAI
from pandasai.llm import OpenAI as pOpenAI


from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image
from reportlab.lib.units import cm

set_pd_engine("pandas")

# OpenAI API Key

# ƒêƒÉng k√Ω font Be VietNam Pro
pdfmetrics.registerFont(TTFont('BeVietNamPro', r'Be_Vietnam_Pro/BeVietnamPro-Light.ttf'))

# Load data
file_path = 'data_dv.csv'  # Replace with your uploaded file path
data = pd.read_csv(file_path)

# Page configuration
st.set_page_config(page_title="ChatBot", layout="wide")

# T√πy ch·ªânh CSS ƒë·ªÉ ƒë·∫©y Markdown s√°t tr√™n c√πng
st.markdown(
    """
    <style>
        .block-container {
            padding-top: 2rem; /* Lo·∫°i b·ªè kho·∫£ng c√°ch ph√≠a tr√™n */
        }
        h1 {
            text-align: center;
            margin-top: 0;
            padding-top: 0;
        }
    </style>
    """,
    unsafe_allow_html=True
)
st.markdown("<h3 style='text-align: center;'>Traffic Accident Advisory ChatBot</h3>", unsafe_allow_html=True)

####################################################
c1, c2 = st.columns([1, 7])
with c1:
    st.image("Animation - 1736347933572.gif")
with c2:
    st.markdown("""
Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi **Traffic Accident Advisory ChatBot**, m·ªôt tr·ª£ l√Ω ·∫£o th√¥ng minh ƒë∆∞·ª£c ph√°t tri·ªÉn ƒë·ªÉ gi√∫p b·∫°n d·ªÖ d√†ng truy v·∫•n th√¥ng tin v√† t·∫°o ra c√°c b√°o c√°o chi ti·∫øt v·ªÅ t√¨nh h√¨nh tai n·∫°n giao th√¥ng ·ªü th√†nh ph·ªë H·ªì Ch√≠ Minh 2020-2021.
D·ª±a tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu phong ph√∫ v√† c·∫≠p nh·∫≠t, ChatBot c√≥ kh·∫£ nƒÉng tr·∫£ l·ªùi nhanh ch√≥ng v√† ch√≠nh x√°c c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn s·ªë l∆∞·ª£ng, lo·∫°i h√¨nh, ƒë·ªãa ƒëi·ªÉm v√† nguy√™n nh√¢n c·ªßa c√°c v·ª• tai n·∫°n giao th√¥ng trong khu v·ª±c.
V·ªõi kh·∫£ nƒÉng ph√¢n t√≠ch d·ªØ li·ªáu m·∫°nh m·∫Ω, ChatBot kh√¥ng ch·ªâ ƒë∆°n gi·∫£n tr·∫£ l·ªùi c√¢u h·ªèi m√† c√≤n c√≥ th·ªÉ t·∫°o ra c√°c b√°o c√°o t√πy ch·ªânh theo y√™u c·∫ßu c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ y√™u c·∫ßu c√°c b√°o c√°o v·ªÅ:
- T√¨nh h√¨nh tai n·∫°n giao th√¥ng theo t·ª´ng nƒÉm, t·ª´ng khu v·ª±c.
- M·ªëi li√™n h·ªá gi·ªØa c√°c y·∫øu t·ªë nh∆∞ th·ªùi ti·∫øt, gi·ªù cao ƒëi·ªÉm v√† s·ªë l∆∞·ª£ng tai n·∫°n.
- Ph√¢n t√≠ch nguy√™n nh√¢n tai n·∫°n v√† c√°c nh√≥m ƒë·ªëi t∆∞·ª£ng li√™n quan.
- D·ª± ƒëo√°n xu h∆∞·ªõng tai n·∫°n giao th√¥ng trong t∆∞∆°ng lai d·ª±a tr√™n c√°c d·ªØ li·ªáu hi·ªán t·∫°i.

**Traffic Accident Advisory ChatBot** s·∫Ω gi√∫p b·∫°n ti·∫øt ki·ªám th·ªùi gian v√† c√¥ng s·ª©c khi t√¨m ki·∫øm th√¥ng tin v√† t·∫°o b√°o c√°o, ƒë·ªìng th·ªùi cung c·∫•p m·ªôt c√¥ng c·ª• h·ªØu √≠ch ƒë·ªÉ n·∫Øm b·∫Øt v√† ph√¢n t√≠ch t√¨nh h√¨nh tai n·∫°n giao th√¥ng m·ªôt c√°ch d·ªÖ d√†ng v√† tr·ª±c quan.""")
####################################################


with st.container(border=True):
    col1, col2 = st.columns(2)
    with col1.container(border=True):
        client1 = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        # from openai import OpenAI as oai

        def load_file_content(file_path):
            try:
                with open(file_path, "r", encoding="utf-8") as file:
                    return file.read().strip()
            except Exception as e:
                st.error(f"L·ªói khi ƒë·ªçc file {file_path}: {e}")
                return ""

        # Load dataset-related information
        dataset_text = load_file_content("./data/dataset_info.txt")
        extra_info = load_file_content("./data/extra_info.txt")
        extra_info_2 = load_file_content("./data/extra_info_2.txt")
        #density = load_file_content("./data/density.txt")
        openai_insights = load_file_content("./data/openai_insights.txt")
        #openai_insights_2 = load_file_content("./data/openai_insights_2.txt")
        openai_summary = load_file_content("./data/openai_summary.txt")


        class StreamlitResponse(ResponseParser):
            def __init__(self, context) -> None:
                super().__init__(context)

            def format_dataframe(self, result):
                st.dataframe(result["value"])
                return

            def format_plot(self, result):
                st.image(result["value"])
                return

            def format_other(self, result):
                st.write(result["value"])
                return

        # df = pd.read_csv('./data/data_dv.csv', encoding="utf-8")

        st.header("Questions and Answers")

        # OPENAI_API_KEY= os.getenv('OPENAI_API_KEY')

        llm = pOpenAI(api_token=os.getenv('OPENAI_API_KEY'))

        # query = st.text_area("üó£Ô∏è Chat with Dataframe")
        query = st.chat_input("Chat with Dataframe >.< ")
        # client = OpenAI(
        #     api_key=OPENAI_API_KEY,
        # )

        def generate_openai_response(client, prompt, model="gpt-4o-mini", max_tokens=1500):
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh, c√≥ kh·∫£ nƒÉng tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn m·ªôt b·ªô d·ªØ li·ªáu l·ªõn."},
                        {"role": "user", "content": prompt}
                    ],
                    #max_tokens=max_tokens,
                    temperature=0.7,
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                return f"L·ªói khi g·ªçi API OpenAI: {e}"

        def process_user_query(client1, query, openai_answer):
            system_prompt = (
                f"""
        B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh v·ªõi kh·∫£ nƒÉng tr·∫£ l·ªùi c√°c c√¢u h·ªèi ph·ª©c t·∫°p li√™n quan ƒë·∫øn b·ªô d·ªØ li·ªáu l·ªõn v√† ph√¢n t√≠ch s√¢u s·∫Øc. B·∫°n c√≥ kh·∫£ nƒÉng gi√∫p ng∆∞·ªùi d√πng hi·ªÉu r√µ h∆°n v·ªÅ c√°c th√¥ng tin v√† c√°c ph√¢n t√≠ch c√≥ s·∫µn, c≈©ng nh∆∞ h·ªó tr·ª£ h·ªç ƒë∆∞a ra c√°c quy·∫øt ƒë·ªãnh ch√≠nh x√°c v√† h·ª£p l√Ω d·ª±a tr√™n d·ªØ li·ªáu.

        **Th√¥ng tin v·ªÅ b·ªô d·ªØ li·ªáu**:
        {dataset_text}

        **Nh·∫≠n x√©t t·ª´ con ng∆∞·ªùi**:
        {extra_info}
        {extra_info_2}

        **Ph√¢n t√≠ch v√† nh·∫≠n x√©t t·ª´ OpenAI**:
        {openai_insights}

        **T√≥m t·∫Øt t·ªïng quan**:
        {openai_summary}

        **Nhi·ªám v·ª• c·ªßa b·∫°n**:
        - X·ª≠ l√Ω v√† ch·ªçn l·ªçc c√°c th√¥ng tin quan tr·ªçng, ƒë·∫∑c bi·ªát ch√∫ tr·ªçng ƒë·∫øn nh·ªØng y·∫øu t·ªë c√≥ t√≠nh tr√πng l·∫∑p cao v√† b·∫£o ƒë·∫£m t√≠nh ch√≠nh x√°c trong c√°c ph√¢n t√≠ch.
        - Cung c·∫•p c√°c d·ª± ƒëo√°n ho·∫∑c gi·∫£i th√≠ch d·ª±a tr√™n d·ªØ li·ªáu c√≥ s·∫µn v√† k·∫øt n·ªëi c√°c ph√¢n t√≠ch v·ªõi c√°c t√¨nh hu·ªëng th·ª±c t·∫ø. ƒê·∫£m b·∫£o r·∫±ng c√°c d·ª± ƒëo√°n n√†y d·ªÖ hi·ªÉu v√† c√≥ th·ªÉ gi√∫p ng∆∞·ªùi d√πng h√¨nh dung r√µ r√†ng v·ªÅ k·∫øt qu·∫£ c√≥ th·ªÉ x·∫£y ra.
        - Ph√¢n t√≠ch v√† k·∫øt n·ªëi c√°c y·∫øu t·ªë d·ªØ li·ªáu ƒë·ªÉ t·∫°o ra nh·ªØng nh·∫≠n x√©t c√≥ chi·ªÅu s√¢u v√† h·ªØu √≠ch cho ng∆∞·ªùi d√πng. ƒê∆∞a ra c√°c nh·∫≠n x√©t b·ªï sung n·∫øu nh·∫≠n th·∫•y r·∫±ng ng∆∞·ªùi d√πng c√≥ th·ªÉ ch∆∞a nh·∫≠n ra m·ªôt m·ªëi li√™n h·ªá quan tr·ªçng n√†o ƒë√≥ trong d·ªØ li·ªáu.
        - Khi tr·∫£ l·ªùi c√¢u h·ªèi, h√£y ƒë·∫£m b·∫£o r·∫±ng c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n r√µ r√†ng, d·ªÖ hi·ªÉu v√† m·∫°ch l·∫°c. ƒê·∫∑c bi·ªát ch√∫ tr·ªçng ƒë·∫øn vi·ªác gi·∫£i quy·∫øt c√°c c√¢u h·ªèi ph·ª©c t·∫°p ho·∫∑c m∆° h·ªì b·∫±ng c√°ch cung c·∫•p c√°c gi·∫£i th√≠ch chi ti·∫øt v√† d·ªÖ ti·∫øp c·∫≠n.
        - N·∫øu c√¢u h·ªèi c√≥ s·ª± m∆° h·ªì ho·∫∑c kh√¥ng r√µ r√†ng, h√£y y√™u c·∫ßu ng∆∞·ªùi d√πng cung c·∫•p th√™m th√¥ng tin v√† l√†m r√µ c√°c y√™u c·∫ßu, ho·∫∑c ƒë·ªÅ xu·∫•t c√°c gi·∫£i ph√°p kh·∫£ thi m√† h·ªç c√≥ th·ªÉ tham kh·∫£o ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n.

        **C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng**: {query}

        # **C√¢u tr·∫£ l·ªùi m·∫´u t·ª´ OpenAI**: {openai_answer}

        H√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt, r√µ r√†ng v√† ch√≠nh x√°c b·∫±ng Ti·∫øng Vi·ªát. ƒê·∫£m b·∫£o r·∫±ng c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n kh√¥ng ch·ªâ ƒë·∫ßy ƒë·ªß m√† c√≤n c√≥ t√≠nh linh ho·∫°t, gi√∫p ng∆∞·ªùi d√πng kh√¥ng ch·ªâ tr·∫£ l·ªùi c√¢u h·ªèi m√† c√≤n kh√°m ph√° th√™m th√¥ng tin c√≥ gi√° tr·ªã t·ª´ b·ªô d·ªØ li·ªáu.
        """
            )
            return generate_openai_response(client1, system_prompt)

        if query:
            query_engine = SmartDataframe(
                data,
                config={
                    "llm": llm,
                    "response_parser": StreamlitResponse,
                    #"custom_whitelisted_dependencies": None,
                    # "callback": StreamlitCallback(container),
                },
            )
            user_query =  f"""
            **C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng**: {query}

            H√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt, r√µ r√†ng v√† ch√≠nh x√°c b·∫±ng Ti·∫øng Vi·ªát. ƒê·∫£m b·∫£o r·∫±ng c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n kh√¥ng ch·ªâ ƒë·∫ßy ƒë·ªß m√† c√≤n c√≥ t√≠nh linh ho·∫°t, gi√∫p ng∆∞·ªùi d√πng kh√¥ng ch·ªâ tr·∫£ l·ªùi c√¢u h·ªèi m√† c√≤n kh√°m ph√° th√™m th√¥ng tin c√≥ gi√° tr·ªã t·ª´ b·ªô d·ªØ li·ªáu.
            """

            openai_answer = query_engine.chat(user_query)
            answer = process_user_query(client1, query, openai_answer)
            st.write(answer)

    ###############################################################################################

    with col2.container(border=True):
        client2 = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

        st.header("Report Generator")
        
        modelfile = f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu v√† bi·ªÉu ƒë·ªì v·ªõi ch·ªß ƒë·ªÅ Tai n·∫°n giao th√¥ng ·ªü Vi·ªát Nam, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng th√¥ng tin dataset ·ªü ƒë√¢y {data}"""

        # User input
        img_file_buffer = st.file_uploader("Th√™m c√°c ·∫£nh b·∫°n mu·ªën ph√¢n t√≠ch v√†o b√°o c√°o", type=["png", "jpg", "jpeg"], accept_multiple_files=True)
        user_input = st.chat_input("Ask me >.< ")

        if user_input:
            st.write("Input")

            introduce_dataset_input = """H√£y vi·∫øt m·ªôt b·∫£n b√°o c√°o gi·ªõi thi·ªáu s∆° l∆∞·ª£c v·ªÅ t·∫≠p d·ªØ li·ªáu tai n·∫°n giao th√¥ng ·ªü Vi·ªát Nam, m√¥ t·∫£ c√°c thu·ªôc t√≠nh c≈©ng nh∆∞ s·ª≠ d·ª•ng c√°c ph√©p to√°n th·ªëng k√™ ƒë∆°n gi·∫£n cho t·∫≠p d·ªØ li·ªáu"""

            result = client2.chat.completions.create(
                            model="gpt-4",
                            messages=[
                                {"role": "system", "content": modelfile},
                                {"role": "user", "content": introduce_dataset_input}
                            ],
                            max_tokens=1500
                        )
            response_text = result.choices[0].message.content
            st.write(response_text)


            ##################################
            # T·∫°o file PDF
            pdf_buffer = io.BytesIO()

            pdf = SimpleDocTemplate(pdf_buffer, pagesize=A4)
            styles = getSampleStyleSheet()
            
            # T√πy ch·ªânh c√°c style
            title_style = ParagraphStyle(
                'TitleStyle', fontName='BeVietNamPro', fontSize=18, alignment=1, spaceAfter=12
            )
            body_style = ParagraphStyle(
                'BodyStyle', fontName='BeVietNamPro', fontSize=12, leading=14, spaceAfter=10
            )
            custom_style = ParagraphStyle(
                'CustomStyle', parent=body_style, alignment=0, spaceAfter=10
            )
            
            # N·ªôi dung PDF
            content = []
            content.append(Paragraph('B√°o C√°o Tai N·∫°n Giao Th√¥ng', title_style))
            content.append(Spacer(1, 0.5 * cm))
            
            # L·∫∑p 3 l·∫ßn ƒë·ªÉ ch√®n vƒÉn b·∫£n v√† ·∫£nh
            paragraphs = response_text.split('\n')
            for para in paragraphs:
                if para.strip():
                    content.append(Paragraph(para.strip(), custom_style))
                    content.append(Spacer(1, 0.2 * cm))
            
            #################################

            st.success("Introduction added to the PDF!")


        # Hi·ªÉn th·ªã h√¨nh ·∫£nh n·∫øu ng∆∞·ªùi d√πng t·∫£i l√™n
            for image in img_file_buffer:
                if image:
                    image = PILImage.open(image)
                    # st.image(image, caption="·∫¢nh ƒë√£ t·∫£i l√™n", width=500)
                    # image = image.resize((512, 200))
                    # image = image.convert("L")

                    buffered = io.BytesIO()
                    image.save(buffered, format="PNG")
                    img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')

                    # X·ª≠ l√Ω h√¨nh ·∫£nh v·ªõi API Ollama (LLaVA)
                    try:
                        # user_input = """T·ª´ t·∫≠p d·ªØ li·ªáu "$(data_dv.csv)" Ph√¢n t√≠ch bi·ªÉu ƒë·ªì n√†y, m√¥ t·∫£ title c·ªßa bi·ªÉu ƒë·ªì, l√† d√≤ng ch·ªØ ph√≠a tr√™n b√™n tr√°i c·ªßa bi·ªÉu ƒë·ªì, ph√≠a sau k√≠ t·ª± 'üìä', m√¥ t·∫£ c√°c tr·ª•c c·ªßa bi·ªÉu ƒë·ªì, m√¥ t·∫£ c√°c ƒëi·ªÉm quan tr·ªçng c·ªßa bi·ªÉu ƒë·ªì, t·ª´ bi·ªÉu ƒë·ªì ƒë√≥ r√∫t ra m√¥ t·∫£ xu h∆∞·ªõng c·ªßa bi·ªÉu ƒë·ªì."""
                        chart_analysis_input = """B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu v√† tr·ª±c quan h√≥a, c√≥ k·ªπ nƒÉng ƒë·ªçc hi·ªÉu bi·ªÉu ƒë·ªì chuy√™n s√¢u. D·ª±a tr√™n bi·ªÉu ƒë·ªì c≈©ng nh∆∞ t·∫≠p d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p, h√£y vi·∫øt m·ªôt b√°o c√°o ph√¢n t√≠ch chi ti·∫øt, r√µ r√†ng v√† m·∫°ch l·∫°c. B√°o c√°o c·∫ßn bao g·ªìm c√°c ph·∫ßn sau:
                                                T·ªïng quan: M√¥ t·∫£ lo·∫°i bi·ªÉu ƒë·ªì, m·ª•c ƒë√≠ch v√† √Ω nghƒ©a t·ªïng qu√°t. X√°c ƒë·ªãnh ch·ªß ƒë·ªÅ ch√≠nh v√† ph·∫°m vi d·ªØ li·ªáu.
                                                Xu h∆∞·ªõng ch√≠nh: Ph√¢n t√≠ch c√°c xu h∆∞·ªõng n·ªïi b·∫≠t, s·ª± thay ƒë·ªïi ƒë√°ng ch√∫ √Ω v√† m·ªëi quan h·ªá gi·ªØa c√°c y·∫øu t·ªë trong bi·ªÉu ƒë·ªì.
                                                Chi ti·∫øt ƒë·∫∑c tr∆∞ng: L√†m r√µ c√°c ƒëi·ªÉm d·ªØ li·ªáu quan tr·ªçng, gi√° tr·ªã cao nh·∫•t, th·∫•p nh·∫•t, v√† c√°c ngo·∫°i l·ªá.
                                                Ph√°t hi·ªán ·∫©n: Nh·∫≠n di·ªán c√°c xu h∆∞·ªõng ho·∫∑c m·∫´u d·ªØ li·ªáu tinh t·∫ø m√† ng∆∞·ªùi xem th√¥ng th∆∞·ªùng c√≥ th·ªÉ b·ªè qua.
                                                K·∫øt lu·∫≠n: T√≥m t·∫Øt l·∫°i c√°c ph√°t hi·ªán quan tr·ªçng, ƒë∆∞a ra nh·∫≠n ƒë·ªãnh t·ªïng th·ªÉ v√† √Ω nghƒ©a t·ª´ bi·ªÉu ƒë·ªì.
                                                S·ª≠ d·ª•ng ng√¥n ng·ªØ chuy√™n nghi·ªáp, ch√≠nh x√°c, c√≥ d·∫´n ch·ª©ng s·ªë li·ªáu c·ª• th·ªÉ t·ª´ bi·ªÉu ƒë·ªì. H√£y ƒë·∫£m b·∫£o ph√¢n t√≠ch to√†n di·ªán, kh√¥ng b·ªè s√≥t th√¥ng tin quan tr·ªçng n√†o v√† r√∫t ra ƒë∆∞·ª£c nh·ªØng k·∫øt lu·∫≠n s√¢u s·∫Øc."""
                        # # Duy·ªát qua t·∫•t c·∫£ c√°c c·ªôt v√† d·ªØ li·ªáu
                        # for col in data.columns:
                        #     user_input += f"{col}: {data[col].tolist()}\n"
                        result = client2.chat.completions.create(
                            model="gpt-4o",
                            messages=[
                                {"role": "system", "content": modelfile},
                                {"role": "user", "content":[
                                    {
                                        "type": "text",
                                        "text": chart_analysis_input
                                    },
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64, {img_base64}"
                                        }
                                    }
                                ]}
                            ],
                            max_tokens=1500
                        )
                        
                        # Save image temporarily
                        with tempfile.NamedTemporaryFile(delete=False, suffix=".png") as tmp_file:
                            image.save(tmp_file, format="PNG")
                            tmp_file_path = tmp_file.name

                            # pil_image = PILImage.new('RGB', (200, 100), color='blue')  # Example image
                            # pil_image.save(tmp_file_path, format="PNG")     

                            f = open(tmp_file_path, 'rb')

                            #################################
                            response_text = result.choices[0].message.content
                            st.write(response_text)

                            content.append(Image(f, width=10*cm, height=6*cm))
                            content.append(Spacer(1, 0.5 * cm))

                            paragraphs = response_text.split('\n')
                            for para in paragraphs:
                                if para.strip():
                                    content.append(Paragraph(para.strip(), custom_style))
                                    content.append(Spacer(1, 0.2 * cm))
                        ##################################

                        # c.drawText(text_object)

                            os.remove(tmp_file_path)
                            st.success("Analysis added to the PDF!")
                    except Exception as e:
                        st.error(f"L·ªói x·ª≠ l√Ω h√¨nh ·∫£nh: {e}")

            pdf.build(content)
            pdf_buffer.seek(0)

            st.write("Done")

            # Provide Download Button
            st.download_button(
                label="Download PDF",
                data=pdf_buffer,
                file_name="output.pdf",
                mime="application/pdf"
            )
